import streamlit as st
from utils import print_messages, StreamHandler
from langchain.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain_core.messages import ChatMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables import RunnablePassthrough
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain import hub
import os

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë…¼ë¬¸ ëŒ€ì‹  ì½ì–´ì¤Œ", page_icon="ğŸ˜")
st.title("ë…¼ë¬¸ ëŒ€ì‹  ì½ì–´ì¤Œ ğŸ˜")

# API key ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"] 

# ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ì½”ë“œ
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì¶œë ¥í•´ì£¼ëŠ” ì½”ë“œ, utils.py íŒŒì¼ë¡œ ë¶„ë¦¬
print_messages()

def get_session_history(session_ids:str) -> BaseChatMessageHistory:
    if session_ids not in st.session_state["store"]:
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids]

def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)

# ì €ì¥í•´ì£¼ê¸°
if "store" not in st.session_state:
    st.session_state["store"] = dict()

# ì„¸ì…˜ ì´ˆê¸°í™” ë° ì„¸ì…˜ ID ì…ë ¥
with st.sidebar:
    session_id = st.text_input("Session ID", value = "abc123")
    
    clear_btn = st.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”")
    if clear_btn:
        st.session_state["messages"] = []
        st.session_state["store"] = dict()
        st.rerun()

# PDF íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ë…¼ë¬¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

# PDF íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆì„ ë•Œ
if uploaded_file is not None:
    with st.spinner("PDF ì²˜ë¦¬ ì¤‘..."):
        # íŒŒì¼ ì €ì¥
        file_path = f"temp_{uploaded_file.name}"
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # PDF ë¡œë“œ
        loader = PyPDFLoader(file_path)
        docs = loader.load()

        # í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
        split_docs = text_splitter.split_documents(docs)

        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings())
        
        # ë²¡í„°ìŠ¤í† ì–´ë¥¼ ê²€ìƒ‰ê¸°ë¡œ ë³€í™˜
        retriever = vectorstore.as_retriever()

        st.success("PDF ì—…ë¡œë“œ ë° ë²¡í„° ì €ì¥ ì™„ë£Œ!")

  
# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ì„œ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œ  
if user_input := st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš©"):
    
    st.chat_message("user").write(f"{user_input}")
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input))
    
    
    # OpenAI Chat APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œ
    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())
        
        # 1. ëª¨ë¸ ìƒì„±
        llm = ChatOpenAI(model_name="gpt-4o", temperature = 0, streaming=True, callbacks=[stream_handler])
        
        # 2. ëŒ€í™” ìƒì„±
        qa_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ì§ˆë¬¸ì— ì§§ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”",
                ),
                
                MessagesPlaceholder(variable_name="history"),
                ("human", "{question}"),
            ]
        )
        
        prompt = hub.pull("rlm/rag-prompt")
          
        chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | qa_prompt 
            | llm
                 )
        
        # 3. ëŒ€í™” ìƒì„±, ë©”ëª¨ë¦¬ ê¸°ëŠ¥ ì¶”ê°€, ìŠ¤íŠ¸ë¦¼ ì¶œë ¥
        chain_with_memory = (
            RunnableWithMessageHistory(
                chain,
                get_session_history,
                input_messages_key="question",
                history_messages_key="history",
            )
        )
        
        # 4. ë‹µë³€ ìƒì„±
        response = chain_with_memory.invoke(
            {"question": user_input},
            config = {"configurable": {"session_id": session_id}}
        )

        # 5. ë‹µë³€ ì¶œë ¥
        msg = response.content
        
        # 6. ëŒ€í™” ê¸°ë¡ ì €ì¥
        st.session_state["messages"].append(ChatMessage(role="assistant", content=msg))
