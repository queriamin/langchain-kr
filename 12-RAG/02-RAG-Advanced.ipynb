{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b82288e",
   "metadata": {},
   "source": [
    "# LangChain의 RAG 파헤치기\n",
    "\n",
    "![rag-1.png](./assets/rag-1.png)\n",
    "\n",
    "![rag-2.png](./assets/rag-2.png)\n",
    "\n",
    "### 1. 질문 처리\n",
    "\n",
    "질문 처리 단계에서는 사용자의 질문을 받아 이를 처리하고, 관련 데이터를 찾는 작업이 이루어집니다. 이를 위해 다음과 같은 구성 요소들이 필요합니다:\n",
    "\n",
    "- **데이터 소스 연결**: 질문에 대한 답변을 찾기 위해 다양한 텍스트 데이터 소스에 연결해야 합니다. LangChain은 다양한 데이터 소스와의 연결을 간편하게 설정할 수 있도록 돕습니다.\n",
    "- **데이터 인덱싱 및 검색**: 데이터 소스에서 관련 정보를 효율적으로 찾기 위해, 데이터는 인덱싱되어야 합니다. LangChain은 인덱싱 과정을 자동화하고, 사용자의 질문과 관련된 데이터를 검색하는 데 필요한 도구를 제공합니다.\n",
    "\n",
    "### 2. 답변 생성\n",
    "\n",
    "관련 데이터를 찾은 후에는 이를 기반으로 사용자의 질문에 답변을 생성해야 합니다. 이 단계에서는 다음 구성 요소가 중요합니다:\n",
    "\n",
    "- **답변 생성 모델**: LangChain은 고급 자연어 처리(NLP) 모델을 사용하여 검색된 데이터로부터 답변을 생성할 수 있는 기능을 제공합니다. 이러한 모델은 사용자의 질문과 검색된 데이터를 입력으로 받아, 적절한 답변을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6f1b5",
   "metadata": {},
   "source": [
    "## 아키텍처\n",
    "\n",
    "우리는 [Q&A 소개](https://python.langchain.comhttps://python.langchain.com/docs/use_cases/question_answering/)에서 개요한 대로 전형적인 RAG 애플리케이션을 만들 것입니다. 이것은 두 가지 주요 구성 요소를 가지고 있습니다:\n",
    "\n",
    "- **인덱싱**: 소스에서 데이터를 수집하고 인덱싱하는 파이프라인입니다. _이 작업은 보통 오프라인에서 발생합니다._\n",
    "\n",
    "- **검색 및 생성**: 실제 RAG 체인으로, 사용자 쿼리를 실행 시간에 받아 인덱스에서 관련 데이터를 검색한 다음, 그 데이터를 모델에 전달합니다.\n",
    "\n",
    "RAW 데이터에서 답변을 받기까지의 전체 순서는 다음과 같습니다.\n",
    "\n",
    "### 인덱싱\n",
    "\n",
    "![](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)\n",
    "\n",
    "1. **로드**: 먼저 데이터를 로드해야 합니다. 이를 위해 [DocumentLoaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)를 사용할 것입니다.\n",
    "2. **분할**: [Text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)는 큰 `Documents`를 더 작은 청크로 나눕니다. 이는 데이터를 인덱싱하고 모델에 전달하는 데 유용하며, 큰 청크는 검색하기 어렵고 모델의 유한한 컨텍스트 창에 맞지 않습니다.\n",
    "3. **저장**: 나중에 검색할 수 있도록 분할을 저장하고 인덱싱할 장소가 필요합니다. 이는 종종 [VectorStore](https://python.langchain.com/docs/modules/data_connection/vectorstores/)와 [Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/) 모델을 사용하여 수행됩니다.\n",
    "\n",
    "### 검색 및 생성\n",
    "\n",
    "![](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)\n",
    "\n",
    "1. **검색**: 사용자 입력이 주어지면 [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/)를 사용하여 저장소에서 관련 분할을 검색합니다.\n",
    "2. **생성**: [ChatModel](https://python.langchain.com/docs/modules/model_io/chat/) / [LLM](https://python.langchain.com/docs/modules/model_io/llms/)은 질문과 검색된 데이터를 포함한 프롬프트를 사용하여 답변을 생성합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf05522",
   "metadata": {},
   "source": [
    "## 실습에 활용한 문서\n",
    "\n",
    "소프트웨어정책연구소(SPRi) - 2023년 12월호\n",
    "\n",
    "- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "- 링크: https://spri.kr/posts/view/23669\n",
    "- 파일명: `SPRI_AI_Brief_2023년12월호_F.pdf`\n",
    "\n",
    "_실습을 위해 다운로드 받은 파일을 `data` 폴더로 복사해 주시기 바랍니다_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c423a8",
   "metadata": {},
   "source": [
    "## 환경설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224fd32",
   "metadata": {},
   "source": [
    "API KEY 를 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418ab505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024d0c5",
   "metadata": {},
   "source": [
    "LangChain으로 구축한 애플리케이션은 여러 단계에 걸쳐 LLM 호출을 여러 번 사용하게 됩니다. 이러한 애플리케이션이 점점 더 복잡해짐에 따라, 체인이나 에이전트 내부에서 정확히 무슨 일이 일어나고 있는지 조사할 수 있는 능력이 매우 중요해집니다. 이를 위한 최선의 방법은 [LangSmith](https://smith.langchain.com)를 사용하는 것입니다.\n",
    "\n",
    "LangSmith가 필수는 아니지만, 유용합니다. LangSmith를 사용하고 싶다면, 위의 링크에서 가입한 후, 로깅 추적을 시작하기 위해 환경 변수를 설정해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edbbf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH12-RAG\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH12-RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d050a",
   "metadata": {},
   "source": [
    "## 모듈별로 자세히 살펴보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11b79b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (2.10.6)\n",
      "Requirement already satisfied: langchain in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-openai in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (0.3.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from pydantic>=2.7.4) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from pydantic>=2.7.4) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from pydantic>=2.7.4) (4.12.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langchain) (0.3.41)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langchain) (2.0.32)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langchain-openai) (1.65.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (8.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.7.24)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade \"pydantic>=2.7.4\" langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d1b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e783c4",
   "metadata": {},
   "source": [
    "아래는 [](https://teddylee777.github.io/langchain/rag-naver-news-qa/) 에서 다뤘던 기본적인 RAG 모델을 사용하는 예제입니다.\n",
    "\n",
    "여기서 각 단계별로 다양한 옵션을 설정하거나 새로운 기법을 적용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377894c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://n.news.naver.com/article/437/0000378416\n",
      "문서의 수: 1\n",
      "============================================================\n",
      "[HUMAN]\n",
      "부영그룹의 출산 장려 정책에 대해 설명해주세요\n",
      "\n",
      "[AI]\n",
      "부영그룹은 출산을 장려하기 위해 2021년 이후 태어난 직원 자녀에 1억원씩, 총 70억원을 지원하고 앞으로도 이 정책을 이어가기로 했습니다. 해당 기간에 연년생과 쌍둥이 자녀가 있으면 총 2억원을 받게 됩니다. 또한, 셋째까지 낳는 경우에는 국민주택을 제공할 예정입니다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 1: 문서 로드(Load Documents)\n",
    "# 뉴스기사 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "url = \"https://n.news.naver.com/article/437/0000378416\"\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url,),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 단계 4: 검색(Search)\n",
    "# 뉴스에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 단계 5: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 단계 6: 언어모델 생성(Create LLM)\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 단계 7: 체인 생성(Create Chain)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"부영그룹의 출산 장려 정책에 대해 설명해주세요\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"URL: {url}\")\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "print(\"===\" * 20)\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b686d9",
   "metadata": {},
   "source": [
    "## 단계 1: 문서 로드(Load Documents)\n",
    "\n",
    "- [공식문서 링크 - Document loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595217a5",
   "metadata": {},
   "source": [
    "### 웹페이지\n",
    "\n",
    "`WebBaseLoader`는 지정된 웹 페이지에서 필요한 부분만을 파싱하기 위해 `bs4.SoupStrainer`를 사용합니다.\n",
    "\n",
    "[참고]\n",
    "\n",
    "- `bs4.SoupStrainer` 는 편리하게 웹에서 원하는 요소를 가져올 수 있도록 해줍니다.\n",
    "\n",
    "(예시)\n",
    "\n",
    "```python\n",
    "bs4.SoupStrainer(\n",
    "    \"div\",\n",
    "    attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]}, # 클래스 명을 입력\n",
    ")\n",
    "\n",
    "bs4.SoupStrainer(\n",
    "    \"article\",\n",
    "    attrs={\"id\": [\"dic_area\"]}, # 클래스 명을 입력\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a040ec3",
   "metadata": {},
   "source": [
    "아래의 BBC 뉴스 기사입니다. 영문으로 작성된 기사로 시험해 보고 싶다면, 아래의 주석을 해제하고 실행해 보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc118579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Could AI \\'trading bots\\' transform the world of investing?1 February 2024ShareSaveJonty BloomBusiness reporterShareSaveGetty ImagesIt is hard for both humans and computers to predict stock market movementsSearch for \"AI investing\" online, and you\\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI \"trading bots\" could apparently do with my investments.Many prominently suggest that they can give me lucrative'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스기사의 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.bbc.com/news/business-68092814\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"main\",\n",
    "            attrs={\"id\": [\"main-content\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43be1cf",
   "metadata": {},
   "source": [
    "### PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14f827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 23\n",
      "\n",
      "[페이지내용]\n",
      "SPRi AI Brief |  \n",
      "2023-12 월호\n",
      "8코히어 , 데이터 투명성 확보를 위한 데이터 출처 탐색기 공개\n",
      "n코히어와 12개 기관이  광범위한 데이터셋에 대한 감사를 통해 원본 데이터 출처, 재라이선스 상태, \n",
      "작성자 등 다양한 정보를 제공하는 ‘데이터 출처 탐색기 ’ 플랫폼을 출시\n",
      "n대화형 플랫폼을 통해 개발자는 데이터셋의 라이선스 상태를 쉽게 파악할 수 있으며 데이터셋의 \n",
      "구성과 계보도 추적 가능KEY Contents\n",
      "£데이터 출처 탐색기 , 광범위한 데이터셋 정보 제공을 통해 데이터 투명성 향상\n",
      "nAI 기업 코히어 (Cohere) 가 매사추세츠 공과⼤(MIT), 하버드 ⼤ 로스쿨 , 카네기멜론 ⼤ 등 12개 기관과  \n",
      "함께 2023 년 10월 25일 ‘데이터 출처 탐색기 (Data Provenance Explorer)’ 플랫폼을 공개\n",
      "∙AI 모델 훈련에 사용되는 데이터셋의 불분명한 출처로 인해 데이터 투명성이 확보되지 않아 다양한 \n",
      "법적·윤리적 문제가 발생\n",
      "∙이에 연구\n",
      "\n",
      "[metadata]\n",
      "{'producer': 'Hancom PDF 1.3.0.542', 'creator': 'Hwp 2018 10.0.0.13462', 'creationdate': '2023-12-08T13:28:38+09:00', 'author': 'dj', 'moddate': '2023-12-08T13:28:38+09:00', 'pdfversion': '1.4', 'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "\n",
    "# 10번째 페이지의 내용 출력\n",
    "print(f\"\\n[페이지내용]\\n{docs[10].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[10].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45216d9",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "CSV 는 페이지 번호 대신 행번호로 데이터를 조회합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77419a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 891\n",
      "\n",
      "[페이지내용]\n",
      "PassengerId: 11\n",
      "Survived: 1\n",
      "Pclass: 3\n",
      "Name: Sandstrom, Miss. Marguerite Rut\n",
      "Sex: female\n",
      "Age: 4\n",
      "SibSp: 1\n",
      "Parch: 1\n",
      "Ticket: PP 9549\n",
      "Fare: 16.7\n",
      "Cabin: G6\n",
      "Embarked: S\n",
      "\n",
      "[metadata]\n",
      "{'source': 'data/titanic.csv', 'row': 10}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# CSV 파일 로드\n",
    "loader = CSVLoader(file_path=\"data/titanic.csv\")\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "\n",
    "# 10번째 페이지의 내용 출력\n",
    "print(f\"\\n[페이지내용]\\n{docs[10].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[10].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4866c8ea",
   "metadata": {},
   "source": [
    "### TXT 파일\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c00113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n",
      "\n",
      "[페이지내용]\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n",
      "\n",
      "[metadata]\n",
      "{'source': 'data/appendix-keywords.txt'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/appendix-keywords.txt\")\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "\n",
    "# 10번째 페이지의 내용 출력\n",
    "print(f\"\\n[페이지내용]\\n{docs[0].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[0].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567b4e0",
   "metadata": {},
   "source": [
    "### 폴더 내의 모든 파일 로드\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133bf8dd",
   "metadata": {},
   "source": [
    "아래는 폴더 내 모든 `.txt` 파일을 로드하는 예시입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fecb15de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]Error loading file data/appendix-keywords-CP949.txt\n",
      " 17%|█▋        | 1/6 [00:03<00:15,  3.13s/it]"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/minjeong/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/share/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DirectoryLoader\n\u001b[1;32m      3\u001b[0m loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m문서의 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 10번째 페이지의 내용 출력\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/directory.py:117\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/directory.py:195\u001b[0m, in \u001b[0;36mDirectoryLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[0;32m--> 195\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_load_file(i, p, pbar)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[1;32m    198\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/directory.py:233\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[0;34m(self, item, path, pbar)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 233\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pbar:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/directory.py:223\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[0;34m(self, item, path, pbar)\u001b[0m\n\u001b[1;32m    221\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_cls(\u001b[38;5;28mstr\u001b[39m(item), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_kwargs)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/unstructured.py:107\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Document]:\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_elements(elements)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/unstructured.py:228\u001b[0m, in \u001b[0;36mUnstructuredFileLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, Path):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstructured_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/auto.py:338\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, skip_infer_table_types, ssl_verify, ocr_languages, languages, pdf_infer_table_structure, xml_keep_tags, data_source_metadata, metadata_filename, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m     elements \u001b[38;5;241m=\u001b[39m partition_image(\n\u001b[1;32m    328\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    329\u001b[0m         file\u001b[38;5;241m=\u001b[39mfile,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    336\u001b[0m     )\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filetype \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mTXT:\n\u001b[0;32m--> 338\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mpartition_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparagraph_grouper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparagraph_grouper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filetype \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mRTF:\n\u001b[1;32m    346\u001b[0m     _partition_rtf \u001b[38;5;241m=\u001b[39m _get_partition_with_extras(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrtf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/documents/elements.py:280\u001b[0m, in \u001b[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[0;32m--> 280\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[1;32m    282\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/file_utils/filetype.py:551\u001b[0m, in \u001b[0;36madd_metadata_with_filetype.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[0;32m--> 551\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[1;32m    553\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/chunking/title.py:257\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[0;32m--> 257\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[1;32m    259\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/text.py:247\u001b[0m, in \u001b[0;36mpartition_text\u001b[0;34m(filename, file, text, encoding, paragraph_grouper, metadata_filename, include_metadata, languages, max_partition, min_partition, metadata_last_modified, chunking_strategy, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_partition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(file_text) \u001b[38;5;241m<\u001b[39m min_partition:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`min_partition` cannot be larger than the length of file contents.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m file_content \u001b[38;5;241m=\u001b[39m \u001b[43msplit_by_paragraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m elements: List[Element] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    254\u001b[0m metadata \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    255\u001b[0m     ElementMetadata(\n\u001b[1;32m    256\u001b[0m         filename\u001b[38;5;241m=\u001b[39mmetadata_filename \u001b[38;5;129;01mor\u001b[39;00m filename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ElementMetadata()\n\u001b[1;32m    262\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/text.py:52\u001b[0m, in \u001b[0;36msplit_by_paragraph\u001b[0;34m(file_text, min_partition, max_partition)\u001b[0m\n\u001b[1;32m     49\u001b[0m split_paragraphs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paragraph \u001b[38;5;129;01min\u001b[39;00m paragraphs:\n\u001b[1;32m     51\u001b[0m     split_paragraphs\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m---> 52\u001b[0m         \u001b[43msplit_content_to_fit_max\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparagraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     58\u001b[0m combined_paragraphs \u001b[38;5;241m=\u001b[39m combine_paragraphs_less_than_min(\n\u001b[1;32m     59\u001b[0m     split_paragraphs\u001b[38;5;241m=\u001b[39msplit_paragraphs,\n\u001b[1;32m     60\u001b[0m     max_partition\u001b[38;5;241m=\u001b[39mmax_partition,\n\u001b[1;32m     61\u001b[0m     min_partition\u001b[38;5;241m=\u001b[39mmin_partition,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m combined_paragraphs\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/text.py:101\u001b[0m, in \u001b[0;36msplit_content_to_fit_max\u001b[0;34m(content, max_partition)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_content_to_fit_max\u001b[39m(\n\u001b[1;32m     96\u001b[0m     content: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     97\u001b[0m     max_partition: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1500\u001b[39m,\n\u001b[1;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Splits a paragraph or section of content so that all of the elements fit into the\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    max partition window.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    103\u001b[0m     tmp_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/nlp/tokenize.py:30\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A wrapper around the NLTK sentence tokenizer with LRU caching enabled.\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m _download_nltk_package_if_not_present(package_category\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, package_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/minjeong/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/share/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\".\", glob=\"data/*.txt\", show_progress=True)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "\n",
    "# 10번째 페이지의 내용 출력\n",
    "print(f\"\\n[페이지내용]\\n{docs[0].page_content[:500]}\")\n",
    "print(f\"\\n[metadata]\\n{docs[0].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9cbb02",
   "metadata": {},
   "source": [
    "다음은 폴더내 모든 `.pdf` 파일을 로드하는 예제입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc40246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error loading file data/SPRI_AI_Brief_2023년12월호_F.pdf\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/minjeong/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/share/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DirectoryLoader\n\u001b[1;32m      3\u001b[0m loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/*.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m문서의 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[메타데이터]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/directory.py:117\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/directory.py:195\u001b[0m, in \u001b[0;36mDirectoryLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[0;32m--> 195\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_load_file(i, p, pbar)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[1;32m    198\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/directory.py:233\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[0;34m(self, item, path, pbar)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 233\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pbar:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/directory.py:223\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[0;34m(self, item, path, pbar)\u001b[0m\n\u001b[1;32m    221\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_cls(\u001b[38;5;28mstr\u001b[39m(item), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_kwargs)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/unstructured.py:107\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Document]:\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_elements(elements)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_community/document_loaders/unstructured.py:228\u001b[0m, in \u001b[0;36mUnstructuredFileLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, Path):\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstructured_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/auto.py:316\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, skip_infer_table_types, ssl_verify, ocr_languages, languages, pdf_infer_table_structure, xml_keep_tags, data_source_metadata, metadata_filename, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filetype \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mPDF:\n\u001b[1;32m    315\u001b[0m     _partition_pdf \u001b[38;5;241m=\u001b[39m _get_partition_with_extras(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 316\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43m_partition_pdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (filetype \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mPNG) \u001b[38;5;129;01mor\u001b[39;00m (filetype \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mJPG) \u001b[38;5;129;01mor\u001b[39;00m (filetype \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mTIFF):\n\u001b[1;32m    327\u001b[0m     elements \u001b[38;5;241m=\u001b[39m partition_image(\n\u001b[1;32m    328\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    329\u001b[0m         file\u001b[38;5;241m=\u001b[39mfile,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    336\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/documents/elements.py:280\u001b[0m, in \u001b[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[0;32m--> 280\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[1;32m    282\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/file_utils/filetype.py:551\u001b[0m, in \u001b[0;36madd_metadata_with_filetype.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[0;32m--> 551\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[1;32m    553\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/chunking/title.py:257\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[0;32m--> 257\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[1;32m    259\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/pdf.py:155\u001b[0m, in \u001b[0;36mpartition_pdf\u001b[0;34m(filename, file, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, max_partition, min_partition, include_metadata, metadata_filename, metadata_last_modified, chunking_strategy, links, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         languages \u001b[38;5;241m=\u001b[39m convert_old_ocr_languages_to_languages(ocr_languages)\n\u001b[1;32m    150\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    151\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe ocr_languages kwarg will be deprecated in a future version of unstructured. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use languages instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    153\u001b[0m         )\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_pdf_or_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/pdf.py:252\u001b[0m, in \u001b[0;36mpartition_pdf_or_image\u001b[0;34m(filename, file, is_image, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, max_partition, min_partition, metadata_last_modified, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m last_modification_date \u001b[38;5;241m=\u001b[39m get_the_last_modification_date_pdf_or_img(\n\u001b[1;32m    237\u001b[0m     file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[1;32m    238\u001b[0m     filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m    239\u001b[0m )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_image\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m determine_pdf_or_image_strategy(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mocr_only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m ):\n\u001b[0;32m--> 252\u001b[0m     extracted_elements \u001b[38;5;241m=\u001b[39m \u001b[43mextractable_elements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspooled_to_bytes_io_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlast_modification_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     pdf_text_extractable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(el, Text) \u001b[38;5;129;01mand\u001b[39;00m el\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m extracted_elements\n\u001b[1;32m    261\u001b[0m     )\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/pdf.py:178\u001b[0m, in \u001b[0;36mextractable_elements\u001b[0;34m(filename, file, include_page_breaks, metadata_last_modified, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    177\u001b[0m     file \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(file)\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_partition_pdf_with_pdfminer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/utils.py:159\u001b[0m, in \u001b[0;36mrequires_dependencies.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_deps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFollowing dependencies are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_deps)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m         ),\n\u001b[1;32m    158\u001b[0m     )\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/pdf.py:430\u001b[0m, in \u001b[0;36m_partition_pdf_with_pdfminer\u001b[0;34m(filename, file, include_page_breaks, metadata_last_modified, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m open_filename(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    429\u001b[0m         fp \u001b[38;5;241m=\u001b[39m cast(BinaryIO, fp)\n\u001b[0;32m--> 430\u001b[0m         elements \u001b[38;5;241m=\u001b[39m \u001b[43m_process_pdfminer_pages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m            \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file:\n\u001b[1;32m    439\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(BinaryIO, file)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/pdf.py:528\u001b[0m, in \u001b[0;36m_process_pdfminer_pages\u001b[0;34m(fp, filename, include_page_breaks, metadata_last_modified, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _text\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[1;32m    527\u001b[0m     points \u001b[38;5;241m=\u001b[39m ((x1, y1), (x1, y2), (x2, y2), (x2, y1))\n\u001b[0;32m--> 528\u001b[0m     element \u001b[38;5;241m=\u001b[39m \u001b[43melement_from_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoordinate_system\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoordinate_system\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m     coordinates_metadata \u001b[38;5;241m=\u001b[39m CoordinatesMetadata(\n\u001b[1;32m    534\u001b[0m         points\u001b[38;5;241m=\u001b[39mpoints,\n\u001b[1;32m    535\u001b[0m         system\u001b[38;5;241m=\u001b[39mcoordinate_system,\n\u001b[1;32m    536\u001b[0m     )\n\u001b[1;32m    538\u001b[0m     links: List[Link] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/text.py:299\u001b[0m, in \u001b[0;36melement_from_text\u001b[0;34m(text, coordinates, coordinate_system)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_possible_numbered_list(text):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListItem(\n\u001b[1;32m    295\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m    296\u001b[0m         coordinates\u001b[38;5;241m=\u001b[39mcoordinates,\n\u001b[1;32m    297\u001b[0m         coordinate_system\u001b[38;5;241m=\u001b[39mcoordinate_system,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_possible_narrative_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NarrativeText(\n\u001b[1;32m    301\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m    302\u001b[0m         coordinates\u001b[38;5;241m=\u001b[39mcoordinates,\n\u001b[1;32m    303\u001b[0m         coordinate_system\u001b[38;5;241m=\u001b[39mcoordinate_system,\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_possible_title(text):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/text_type.py:77\u001b[0m, in \u001b[0;36mis_possible_narrative_text\u001b[0;34m(text, cap_threshold, non_alpha_threshold, languages, language_checks)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# NOTE(robinson): it gets read in from the environment as a string so we need to\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# cast it to a float\u001b[39;00m\n\u001b[1;32m     74\u001b[0m cap_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\n\u001b[1;32m     75\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNSTRUCTURED_NARRATIVE_TEXT_CAP_THRESHOLD\u001b[39m\u001b[38;5;124m\"\u001b[39m, cap_threshold),\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mexceeds_cap_ratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcap_threshold\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     78\u001b[0m     trace_logger\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot narrative. Text exceeds cap ratio \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcap_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore # noqa: E501\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/text_type.py:274\u001b[0m, in \u001b[0;36mexceeds_cap_ratio\u001b[0;34m(text, threshold)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks the title ratio in a section of text. If a sufficient proportion of the words\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03mare capitalized, that can be indicated on non-narrative text (i.e. \"1A. Risk Factors\").\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    the function returns True\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# NOTE(robinson) - Currently limiting this to only sections of text with one sentence.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# The assumption is that sections with multiple sentences are not titles.\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msentence_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39misupper():\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/partition/text_type.py:222\u001b[0m, in \u001b[0;36msentence_count\u001b[0;34m(text, min_length)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_count\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, min_length: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Checks the sentence count for a section of text. Titles should not be more than one\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    sentence.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m        The min number of words a section needs to be for it to be considered a sentence.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/unstructured/nlp/tokenize.py:30\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A wrapper around the NLTK sentence tokenizer with LRU caching enabled.\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m _download_nltk_package_if_not_present(package_category\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, package_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/minjeong/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/share/nltk_data'\n    - '/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\".\", glob=\"data/*.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"문서의 수: {len(docs)}\\n\")\n",
    "print(\"[메타데이터]\\n\")\n",
    "print(docs[0].metadata)\n",
    "print(\"\\n========= [앞부분] 미리보기 =========\\n\")\n",
    "print(docs[0].page_content[2500:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3065d6",
   "metadata": {},
   "source": [
    "### Python\n",
    "\n",
    "다음은 `.py` 파일을 로드하는 예제입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36678ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n",
      "\n",
      "[메타데이터]\n",
      "\n",
      "{'source': 'data/audio_utils.py'}\n",
      "\n",
      "========= [앞부분] 미리보기 =========\n",
      "\n",
      "import re\n",
      "import os\n",
      "from pytube import YouTube\n",
      "from moviepy.editor import AudioFileClip, VideoFileClip\n",
      "from pydub import AudioSegment\n",
      "from pydub.silence import detect_nonsilent\n",
      "\n",
      "\n",
      "def extract_abr(abr):\n",
      "    youtube_audio_pattern = re.compile(r\"\\d+\")\n",
      "    kbps = youtube_audio_pattern.search(abr)\n",
      "    if kbps:\n",
      "        kbps = kbps.group()\n",
      "        return int(kbps)\n",
      "    else:\n",
      "        return 0\n",
      "\n",
      "\n",
      "def get_audio_filepath(filename):\n",
      "    # audio 폴더가 없으면 생성\n",
      "    if not os.path.isdir(\"audio\"):\n",
      "        os.mkdir(\"au\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "\n",
    "loader = DirectoryLoader(\".\", glob=\"**/*.py\", loader_cls=PythonLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"문서의 수: {len(docs)}\\n\")\n",
    "print(\"[메타데이터]\\n\")\n",
    "print(docs[0].metadata)\n",
    "print(\"\\n========= [앞부분] 미리보기 =========\\n\")\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505bd21b",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0321a46",
   "metadata": {},
   "source": [
    "## 단계 2: 문서 분할(Split Documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ad4770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Could AI \\'trading bots\\' transform the world of investing?1 February 2024ShareSaveJonty BloomBusiness reporterShareSaveGetty ImagesIt is hard for both humans and computers to predict stock market movementsSearch for \"AI investing\" online, and you\\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI \"trading bots\" could apparently do with my investments.Many prominently suggest that they can give me lucrative'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스기사의 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.bbc.com/news/business-68092814\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"main\",\n",
    "            attrs={\"id\": [\"main-content\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc11cb",
   "metadata": {},
   "source": [
    "### CharacterTextSplitter\n",
    "\n",
    "이것은 가장 간단한 방법입니다. 이 방법은 문자를 기준으로 분할합니다(기본값은 \"\\n\\n\") 그리고 청크의 길이를 문자의 수로 측정합니다.\n",
    "\n",
    "1. 텍스트가 어떻게 분할되는지: 단일 문자 단위\n",
    "2. 청크 크기가 어떻게 측정되는지: `len` of characters.\n",
    "\n",
    "시각화 예제: https://chunkviz.up.railway.app/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4a799",
   "metadata": {},
   "source": [
    "`CharacterTextSplitter` 클래스는 텍스트를 특정 크기의 청크로 분할하는 기능을 제공합니다.\n",
    "\n",
    "- `separator` 매개변수는 청크를 구분하는 데 사용되는 문자열을 지정하며, 여기서는 두 개의 개행 문자(`\"\\n\\n\"`)를 사용합니다\n",
    "- `chunk_size`는 각 청크의 최대 길이를 결정합니다\n",
    "- `chunk_overlap`은 인접한 청크 간에 겹치는 문자의 수를 지정합니다.\n",
    "- `length_function`은 청크의 길이를 계산하는 데 사용되는 함수를 결정하며, 기본적으로 문자열의 길이를 반환하는 `len` 함수가 사용됩니다.\n",
    "- `is_separator_regex`는 `separator`가 정규 표현식으로 해석될지 여부를 결정하는 불리언 값입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ade83078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d7e0d",
   "metadata": {},
   "source": [
    "이 함수는 `text_splitter` 객체의 `create_documents` 메소드를 사용하여 주어진 텍스트(`state_of_the_union`)를 여러 문서로 분할하고, 그 결과를 `texts` 변수에 저장합니다. 이후 `texts`의 첫 번째 문서를 출력합니다. 이 과정은 텍스트 데이터를 처리하고 분석하기 위한 초기 단계로 볼 수 있으며, 특히 큰 텍스트 데이터를 관리 가능한 크기의 단위로 나누는 데 유용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "472bc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain of density 논문의 일부 내용을 불러옵니다\n",
    "with open(\"data/chain-of-density.txt\", \"r\") as f:\n",
    "    text = f.read()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70e11f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task. \\nA good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries genera']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separator=\"\\n\\n\"\n",
    ")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24506d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task.',\n",
       " 'A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries genera']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\"\\n\")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57da16a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task. \\nA good',\n",
       " 'A good summary should be detailed and entity-centric without being overly dense and hard to follow.',\n",
       " 'to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with',\n",
       " 'with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial',\n",
       " 'an initial entity-sparse summary before iteratively incorporating missing salient entities without',\n",
       " 'without increasing the length. Summaries genera']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator=\" \")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c743226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecting the “right” amount of information to include in a summary is a difficult task. \\nA good',\n",
       " 'summary should be detailed and entity-centric without being overly dense and hard to follow. To',\n",
       " 'better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to',\n",
       " 'as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary',\n",
       " 'before iteratively incorporating missing salient entities without increasing the length. Summaries',\n",
       " 'genera']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0, separator=\" \")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b5b02d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \")\n",
    "# text 파일을 청크로 나누어줍니다.\n",
    "text_splitter.split_text(text)\n",
    "\n",
    "# document를 청크로 나누어줍니다.\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2f39fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.bbc.com/news/business-68092814'}, page_content='Could AI \\'trading bots\\' transform the world of investing?1 February 2024ShareSaveJonty BloomBusiness reporterShareSaveGetty ImagesIt is hard for both humans and computers to predict stock market movementsSearch for \"AI investing\" online, and you\\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI \"trading bots\" could apparently do with my investments.Many prominently suggest that they can give me lucrative returns. Yet as every reputable financial firm warns - your capital may be at risk.Or putting it more simply - you could lose your money - whether it is a human or a computer that is making stock market decisions on your behalf.Yet such has been the hype about the ability of AI over the past few years, that almost one in three investors would be happy to let a trading bot make all the decisions for them, according to one 2023 survey in the US.John Allan says investors should be more cautious')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e376c2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Could AI \\'trading bots\\' transform the world of investing?1 February 2024ShareSaveJonty BloomBusiness reporterShareSaveGetty ImagesIt is hard for both humans and computers to predict stock market movementsSearch for \"AI investing\" online, and you\\'ll be flooded with endless offers to let artificial intelligence manage your money.I recently spent half an hour finding out what so-called AI \"trading bots\" could apparently do with my investments.Many prominently suggest that they can give me lucrative'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 뉴스기사의 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://www.bbc.com/news/business-68092814\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"main\",\n",
    "            attrs={\"id\": [\"main-content\"]},\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# splitter 를 정의합니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \")\n",
    "\n",
    "# 문서를 로드시 바로 분할까지 수행합니다.\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3662cd",
   "metadata": {},
   "source": [
    "### RecursiveTextSplitter\n",
    "\n",
    "이 텍스트 분할기는 일반 텍스트에 권장되는 텍스트 분할기입니다.\n",
    "\n",
    "1. 텍스트가 어떻게 분할 규칙: list of `separators`\n",
    "2. 청크 크기가 어떻게 측정되는가: `len` of characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f33726f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain 패키지에서 RecursiveCharacterTextSplitter 클래스를 가져옵니다.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a2dad",
   "metadata": {},
   "source": [
    "`RecursiveCharacterTextSplitter` 클래스는 텍스트를 재귀적으로 분할하는 기능을 제공합니다. 이 클래스는 `chunk_size`로 분할할 청크의 크기, `chunk_overlap`으로 인접 청크 간의 겹침 크기, `length_function`으로 청크의 길이를 계산하는 함수, 그리고 `is_separator_regex`로 구분자가 정규 표현식인지 여부를 지정하는 매개변수를 받습니다. 예시에서는 청크 크기를 100, 겹침 크기를 20으로 설정하고, 길이 계산 함수로 `len`을 사용하며, 구분자가 정규 표현식이 아님을 나타내기 위해 `is_separator_regex`를 `False`로 설정합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6483dace",
   "metadata": {},
   "source": [
    "정규 표현식(Regex)은 특정한 패턴의 문자열을 찾거나 분할하는 데 사용하는 강력한 도구야.\n",
    "RecursiveCharacterTextSplitter에서 텍스트를 분할할 때 단순한 문자(예: .) 대신 정규 표현식(예: \\s{2,})을 사용할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fc0587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # 정말 작은 청크 크기를 설정합니다.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a8cf631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain of density 논문의 일부 내용을 불러옵니다\n",
    "with open(\"data/chain-of-density.txt\", \"r\") as f:\n",
    "    text = f.read()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26f2bb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting the “right” amount of information to include in a summary is a difficult task. \n",
      "A good\n",
      "A good summary should be detailed and entity-centric without being overly dense and hard to follow.\n",
      "to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with\n",
      "with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial\n",
      "an initial entity-sparse summary before iteratively incorporating missing salient entities without\n",
      "without increasing the length. Summaries genera\n",
      "============================================================\n",
      "Selecting the “right” amount of information to include in a summary is a difficult task.\n",
      "A good summary should be detailed and entity-centric without being overly dense and hard to follow.\n",
      "follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what\n",
      "with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an\n",
      "an initial entity-sparse summary before iteratively incorporating missing salient entities without\n",
      "without increasing the length. Summaries genera\n"
     ]
    }
   ],
   "source": [
    "character_text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10, separator=\" \"\n",
    ")\n",
    "for sent in character_text_splitter.split_text(text):\n",
    "    print(sent)\n",
    "print(\"===\" * 20)\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=10\n",
    ")\n",
    "for sent in recursive_text_splitter.split_text(text):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264cf4ae",
   "metadata": {},
   "source": [
    "- 지정한 separators 리스트를 순차적으로 시도하며 주어진 문서를 분할합니다.\n",
    "- 청크가 충분히 작아질 때까지 순서대로 분할을 시도합니다. 기본 목록은 [\"\\n\\n\", \"\\n\", \" \", \"\"]입니다.\n",
    "- 이는 일반적으로 의미적으로 가장 연관성이 강한 텍스트 조각인 것처럼 보이는 모든 단락(그리고 문장, 단어)을 가능한 한 길게 유지하려는 효과가 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "393ec4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recursive_text_splitter 에 기본 지정된 separators 를 확인합니다.\n",
    "recursive_text_splitter._separators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49d975",
   "metadata": {},
   "source": [
    "### Semantic Similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05729bc0",
   "metadata": {},
   "source": [
    "의미적 유사성을 기준으로 텍스트를 분할합니다.\n",
    "\n",
    "출처: [Greg Kamradt’s Notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n",
    "\n",
    "높은 수준(high level)에서 문장으로 분할한 다음 3개 문장으로 그룹화한 다음 임베딩 공간에서 유사한 문장을 병합하는 방식입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "206ff657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 최신 버전으로 업데이트합니다.\n",
    "%pip install -U langchain langchain_experimental -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dccff243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# SemanticChunker 를 생성합니다.\n",
    "semantic_text_splitter = SemanticChunker(OpenAIEmbeddings(), add_start_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a27b2d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting the “right” amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt. We conduct a human preference study on 100 CNN DailyMail articles and find that that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries. Qualitative analysis supports the notion that there exists a tradeoff between infor-mativeness and readability. 500 annotated CoD summaries, as well as an extra 5,000 unannotated summaries, are freely available on HuggingFace. Introduction\n",
      "\n",
      "Automatic summarization has come a long way in the past few years, largely due to a paradigm shift away from supervised fine-tuning on labeled datasets to zero-shot prompting with Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023). Without additional training, careful prompting can enable fine-grained control over summary characteristics, such as length (Goyal et al., 2022), topics (Bhaskar et al., 2023), and style (Pu and Demberg, 2023).\n",
      "============================================================\n",
      "An overlooked aspect is the information density of an summary. In theory, as a compression of another text, a summary should be denser–containing a higher concentration of information–than the source document. Given the high latency of LLM decoding (Kad-dour et al., 2023), covering more information in fewer words is a worthy goal, especially for real-time applications. Yet, how dense is an open question. A summary is uninformative if it contains insufficient detail. If it contains too much information, however, it can be-come difficult to follow without having to increase the overall length. Conveying more information subject to a fixed token budget requires a combination of abstrac-tion, compression, and fusion. There is a limit to how much space can be made for additional information before becoming illegible or even factually incorrect.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# chain of density 논문의 일부 내용을 불러옵니다\n",
    "with open(\"data/chain-of-density.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "for sent in semantic_text_splitter.split_text(text):\n",
    "    print(sent)\n",
    "    print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f3467",
   "metadata": {},
   "source": [
    "## 3 단계: 임베딩\n",
    "\n",
    "참고: https://python.langchain.com/docs/integrations/text_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9affbaf8",
   "metadata": {},
   "source": [
    "### 유료 과금 임베딩(OpenAI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc62a14",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 벡터스토어를 생성합니다.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(documents\u001b[38;5;241m=\u001b[39m\u001b[43msplits\u001b[49m, embedding\u001b[38;5;241m=\u001b[39mOpenAIEmbeddings())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'splits' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65e243",
   "metadata": {},
   "source": [
    "다음은 `OpenAI` 의 지원되는 Embedding 모델들의 목록입니다.\n",
    "\n",
    "- 기본 값은 `text-embeding-ada-002` 입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c021b1",
   "metadata": {},
   "source": [
    "| MODEL                  | ROUGH PAGES PER DOLLAR | EXAMPLE PERFORMANCE ON MTEB EVAL |\n",
    "| ---------------------- | ---------------------- | -------------------------------- |\n",
    "| text-embedding-3-small | 62,500                 | 62.3%                            |\n",
    "| text-embedding-3-large | 9,615                  | 64.6%                            |\n",
    "| text-embedding-ada-002 | 12,500                 | 61.0%                            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cf6e8",
   "metadata": {},
   "source": [
    "### 무료 Open Source 기반 임베딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ca55cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/7c5_hjk95zd19_dhwzjskkfr0000gn/T/ipykernel_3951/3999565279.py:6: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  documents=splits, embedding=HuggingFaceBgeEmbeddings()\n",
      "/var/folders/1y/7c5_hjk95zd19_dhwzjskkfr0000gn/T/ipykernel_3951/3999565279.py:6: LangChainDeprecationWarning: Default values for HuggingFaceBgeEmbeddings.model_name were deprecated in LangChain 0.2.5 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceBgeEmbeddings constructor instead.\n",
      "  documents=splits, embedding=HuggingFaceBgeEmbeddings()\n",
      "/Users/minjeong/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=splits, embedding=HuggingFaceBgeEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2964bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fastembed -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d13003a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620e417c53f64dc1848ec3cc5338b329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218b191fbfc64806ac39970cecd4e2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_optimized.onnx:   0%|          | 0.00/66.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b43f541c6074df0bfebb449f99a0198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57d1626154846bb82e92fcc5935059f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddec74638ff4ca7a2a8d277def29e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cc5c7e2a804f92a0f50b668abb5444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=FastEmbedEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd42083c",
   "metadata": {},
   "source": [
    "## 4단계: 벡터스토어 생성(Create Vectorstore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9033e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# FAISS DB 적용\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0a99a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Chroma DB 적용\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8150225",
   "metadata": {},
   "source": [
    "## 5단계: Retriever 생성\n",
    "\n",
    "리트리버는 구조화되지 않은 쿼리가 주어지면 문서를 반환하는 인터페이스입니다.\n",
    "\n",
    "리트리버는 문서를 저장할 필요 없이 문서를 반환(또는 검색)하기만 합니다.\n",
    "\n",
    "- [공식 도큐먼트](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n",
    "\n",
    "생성된 VectorStore 에 `as_retriver()` 로 가져와서 **Retriever** 를 생성합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90798e85",
   "metadata": {},
   "source": [
    "### 유사도 기반 검색\n",
    "\n",
    "- 기본값은 코사인 유사도인 `similarity` 가 적용되어 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2bd6d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/7c5_hjk95zd19_dhwzjskkfr0000gn/T/ipykernel_3951/1213931718.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  search_result = retriever.get_relevant_documents(query)\n",
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}, page_content=\"출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책\"), Document(metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}, page_content='남성 직원의 육아휴직을 의무화한 곳도 있습니다.사내 어린이집을 밤 10시까지 운영하고 셋째를 낳으면 무조건 승진시켜 주기도 합니다.한 회사는 지난해 네쌍둥이를 낳은 직원에 의료비를 지원해 관심을 모았습니다.정부 대신 회사가 나서는 출산장려책이 사회적 분위기를 바꿀 거라는 기대가 커지는 가운데, 여력이 부족한 중소지원이 필요하다는 목소리도 나옵니다.[영상디자인 곽세미]'), Document(metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}, page_content=\"[앵커]올해 아이 낳을 계획이 있는 가족이라면 솔깃할 소식입니다. 정부가 저출생 대책으로 매달 주는 부모 급여, 0세 아이는 100만원으로 올렸습니다. 여기에 첫만남이용권, 아동수당까지 더하면 아이 돌까지 1년 동안 1520만원을 받습니다. 지자체도 경쟁하듯 지원에 나섰습니다. 인천시는 새로 태어난 아기, 18살될 때까지 1억원을 주겠다. 광주시도 17살될 때까지 7400만원 주겠다고 했습니다. 선거 때면 나타나서 아이 낳으면 현금 주겠다고 밝힌 사람이 있었죠. 과거에는 표만 노린 '황당 공약'이라는 비판이 따라다녔습니다. 그런데 지금은 출산율이 이보다 더 나쁠 수 없다보니, 이런 현금성 지원을 진지하게 정책화 하는 상황까지 온 겁니다. 게다가 기업들도 뛰어들고 있습니다. 이번에는 출산한 직원에게 단번에 1억원을 주겠다는 회사까지 나타났습니다.이상화 기자가 취재했습니다.[기자]한 그룹사가 오늘 파격적인 저출생 정책을 내놨습니다.2021년 이후 태어난 직원 자녀에 1억원씩, 총 70억원을 지원하고 앞으로도 이 정책을 이어가기로 했습니다.해당 기간에 연년생과 쌍둥이 자녀가 있으면 총 2억원을 받게 됩니다.[오현석/부영그룹 직원 : 아이 키우는 데 금전적으로 많이 힘든 세상이잖아요. 교육이나 생활하는 데 큰 도움이 될 거라 생각합니다.]만약 셋째까지 낳는 경우엔 국민주택을 제공하겠다는 뜻도 밝혔습니다.[이중근/부영그룹 회장 : 3년 이내에 세 아이를 갖는 분이 나올 것이고 따라서 주택을 제공할 수 있는 계기가 될 것으로 생각하고.][조용현/부영그룹 직원 : 와이프가 셋째도 갖고 싶어 했는데 경제적 부담 때문에 부정적이었거든요. (이제) 긍정적으로 생각할 수 있을 것 같습니다.]오늘 행사에서는, 회사가 제공하는 출산장려금은 받는 직원들의 세금 부담을 고려해 정부가 면세해달라는 제안도 나왔습니다.이같은 출산장려책은 점점 확산하는 분위기입니다.법정기간보다 육아휴직을 길게 주거나, 남성 직원의 육아휴직을 의무화한 곳도 있습니다.사내 어린이집을 밤 10시까지 운영하고\")]\n"
     ]
    }
   ],
   "source": [
    "query = \"회사의 저출생 정책이 뭐야?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107dea5",
   "metadata": {},
   "source": [
    "`similarity_score_threshold` 는 유사도 기반 검색에서 `score_threshold` 이상인 결과만 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "067f05aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}, page_content=\"출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책\")]\n"
     ]
    }
   ],
   "source": [
    "query = \"회사의 저출생 정책이 뭐야?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.8}\n",
    ")\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72fee8",
   "metadata": {},
   "source": [
    "`maximum marginal search result` 를 사용하여 검색합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "405337a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}, page_content=\"출산 직원에게 '1억원' 쏜다…회사의 파격적 저출생 정책\"), Document(metadata={'source': 'https://n.news.naver.com/article/437/0000378416'}, page_content='남성 직원의 육아휴직을 의무화한 곳도 있습니다.사내 어린이집을 밤 10시까지 운영하고 셋째를 낳으면 무조건 승진시켜 주기도 합니다.한 회사는 지난해 네쌍둥이를 낳은 직원에 의료비를 지원해 관심을 모았습니다.정부 대신 회사가 나서는 출산장려책이 사회적 분위기를 바꿀 거라는 기대가 커지는 가운데, 여력이 부족한 중소지원이 필요하다는 목소리도 나옵니다.[영상디자인 곽세미]')]\n"
     ]
    }
   ],
   "source": [
    "query = \"회사의 저출생 정책이 뭐야?\"\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})\n",
    "search_result = retriever.get_relevant_documents(query)\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a96b86",
   "metadata": {},
   "source": [
    "### 다양한 쿼리 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3473f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "query = \"회사의 저출생 정책이 뭐야?\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e743451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd0aed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. 부영그룹이 채택한 출산 장려 정책은 무엇인가요?', '2. 부영그룹의 출산 촉진 정책은 어떻게 운영되고 있나요?', '3. 부영그룹이 시행하는 출산 지원 정책에 대해 자세히 알려주세요.']\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8185cf7",
   "metadata": {},
   "source": [
    "### Ensemble Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd02b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52988fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [\n",
    "    \"난 오늘 많이 먹어서 배가 정말 부르다\",\n",
    "    \"떠나는 저 배가 오늘 마지막 배인가요?\",\n",
    "    \"내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\",\n",
    "]\n",
    "\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(doc_list)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "faiss_vectorstore = FAISS.from_texts(doc_list, OpenAIEmbeddings())\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6658f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"[{i+1}] {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f046a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "나 요즘 배에 정말 살이 많이 쪘어...\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] 난 오늘 많이 먹어서 배가 정말 부르다\n",
      "[2] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] 난 오늘 많이 먹어서 배가 정말 부르다\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] 난 오늘 많이 먹어서 배가 정말 부르다\n",
      "[2] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[3] 떠나는 저 배가 오늘 마지막 배인가요?\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"나 요즘 배에 정말 살이 많이 쪘어...\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99d87210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "바다 위에 떠다니는 배들이 많다\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] 난 오늘 많이 먹어서 배가 정말 부르다\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "[2] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[3] 난 오늘 많이 먹어서 배가 정말 부르다\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"바다 위에 떠다니는 배들이 많다\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60a3345b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "ships\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "[2] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"ships\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2210c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]\n",
      "pear\n",
      "\n",
      "[BM25 Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[FAISS Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n",
      "============================================================\n",
      "[Ensemble Retriever]\n",
      "[1] 내가 제일 좋아하는 과일들은 배, 사과, 키워, 수박 입니다.\n",
      "[2] 떠나는 저 배가 오늘 마지막 배인가요?\n"
     ]
    }
   ],
   "source": [
    "sample_query = \"pear\"\n",
    "print(f\"[Query]\\n{sample_query}\\n\")\n",
    "relevant_docs = bm25_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[BM25 Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = faiss_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[FAISS Retriever]\")\n",
    "pretty_print(relevant_docs)\n",
    "print(\"===\" * 20)\n",
    "relevant_docs = ensemble_retriever.get_relevant_documents(sample_query)\n",
    "print(\"[Ensemble Retriever]\")\n",
    "pretty_print(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9d842",
   "metadata": {},
   "source": [
    "## 6단계: 프롬프트 생성(Create Prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d94fcc",
   "metadata": {},
   "source": [
    "프롬프트 엔지니어링은 주어진 데이터(`context`)를 토대로 우리가 원하는 결과를 도출할 때 중요한 역할을 합니다.\n",
    "\n",
    "[TIP1]\n",
    "\n",
    "1. 만약, `retriever` 에서 도출한 결과에서 중요한 정보가 누락된다면 `retriever` 의 로직을 수정해야 합니다.\n",
    "2. 만약, `retriever` 에서 도출한 결과가 많은 정보를 포함하고 있지만, `llm` 이 그 중에서 중요한 정보를 찾지 못한거나 원하는 형태로 출력하지 않는다면 프롬프트를 수정해야 합니다.\n",
    "\n",
    "[TIP2]\n",
    "\n",
    "1. LangSmith 의 **hub** 에는 검증된 프롬프트가 많이 업로드 되어 있습니다.\n",
    "2. 검증된 프롬프트를 활용하거나 약간 수정한다면 비용과 시간을 절약할 수 있습니다.\n",
    "\n",
    "- https://smith.langchain.com/hub/search?q=rag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20486a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61e05003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d8714",
   "metadata": {},
   "source": [
    "## 7단계: 언어모델 생성(Create LLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f99381",
   "metadata": {},
   "source": [
    "OpenAI 모델 중 하나를 선택합니다.\n",
    "\n",
    "- `gpt-3.5-turbo` : OpenAI의 GPT-3.5-turbo 모델\n",
    "- `gpt-4-turbo-preview` : OpenAI의 GPT-4-turbo-preview 모델\n",
    "\n",
    "자세한 비용 체계는 [OpenAI API 모델 리스트 / 요금표](https://teddylee777.github.io/openai/openai-models/)에서 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85dcaa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64e9a5",
   "metadata": {},
   "source": [
    "다음의 방식으로 토큰 사용량을 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f087fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 40\n",
      "\tPrompt Tokens: 24\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 16\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $3.6e-05\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = model.invoke(\"대한민국의 수도는 어디인가요?\")\n",
    "print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6cd40",
   "metadata": {},
   "source": [
    "HuggingFace 에 업로드 되어 있는 오픈소스 모델을 손쉽게 다운로드 받아 사용할 수 있습니다.\n",
    "\n",
    "아래의 리더보드에서 날마다 성능을 개선하는 오픈소스 리더보드를 확인할 수 있습니다.\n",
    "\n",
    "- [HuggingFace LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba46d84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/7c5_hjk95zd19_dhwzjskkfr0000gn/T/ipykernel_3951/3291091178.py:6: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  t5_model = HuggingFaceHub(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHub\n  Value error, Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. [type=value_error, input_value={'repo_id': 'google/flan-...acehub_api_token': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceHub\n\u001b[1;32m      4\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-xxl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m t5_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceHub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:214\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     emit_warning()\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-szXyjQUn-py3.11/lib/python3.11/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\n  Value error, Did not find huggingfacehub_api_token, please add an environment variable `HUGGINGFACEHUB_API_TOKEN` which contains it, or pass `huggingfacehub_api_token` as a named parameter. [type=value_error, input_value={'repo_id': 'google/flan-...acehub_api_token': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/value_error"
     ]
    }
   ],
   "source": [
    "# HuggingFaceHub 객체 생성\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = \"google/flan-t5-xxl\"\n",
    "\n",
    "t5_model = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "628f91f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [05:28<27:22, 328.58s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 't5_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mt5_model\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere is the capital of South Korea?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 't5_model' is not defined"
     ]
    }
   ],
   "source": [
    "t5_model.invoke(\"Where is the capital of South Korea?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74046f",
   "metadata": {},
   "source": [
    "## RAG 템플릿 실험\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88c64211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Path: data/SPRI_AI_Brief_2023년12월호_F.pdf\n",
      "문서의 수: 1\n",
      "============================================================\n",
      "[HUMAN]\n",
      "부영그룹의 출산 장려 정책에 대해 설명해주세요\n",
      "\n",
      "[AI]\n",
      "부영그룹은 출산을 장려하기 위해 2021년 이후 태어난 직원 자녀에 1억원씩, 총 70억원을 지원하고 앞으로도 이 정책을 이어가기로 했습니다. 해당 기간에 연년생과 쌍둥이 자녀가 있으면 총 2억원을 받게 됩니다. 또한, 셋째까지 낳는 경우에는 국민주택을 제공할 예정입니다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 1: 문서 로드(Load Documents)\n",
    "# 문서를 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "file_path = \"data/SPRI_AI_Brief_2023년12월호_F.pdf\"\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "# 단계 3, 4: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 단계 5: 리트리버 생성(Create Retriever)\n",
    "# 사용자의 질문(query) 에 부합하는 문서를 검색합니다.\n",
    "\n",
    "# 유사도 높은 K 개의 문서를 검색합니다.\n",
    "k = 3\n",
    "\n",
    "# (Sparse) bm25 retriever and (Dense) faiss retriever 를 초기화 합니다.\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "bm25_retriever.k = k\n",
    "\n",
    "faiss_vectorstore = FAISS.from_documents(split_docs, OpenAIEmbeddings())\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 단계 7: 언어모델 생성(Create LLM)\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 단계 8: 체인 생성(Create Chain)\n",
    "rag_chain = (\n",
    "    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"PDF Path: {file_path}\")\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "print(\"===\" * 20)\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349c363",
   "metadata": {},
   "source": [
    "> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 10)\n",
    "\n",
    "- LangSmith: https://smith.langchain.com/public/4449e744-f0a0-42d2-a3df-855bd7f41652/r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "218d2974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자가 자체 개발한 생성 AI 모델 '삼성 가우스'는 온디바이스에서 작동 가능하며 언어, 코드, 이미지의 3개 모델로 구성되어 있습니다. 삼성 가우스는 외부로 사용자 정보가 유출될 위험이 없는 안전한 데이터를 통해 학습되었고, 다양한 상황에 최적화된 크기의 모델 선택이 가능합니다. 이 모델은 텍스트, 코드, 이미지를 생성하는 세 가지 모델로 구성되어 있으며, 다양한 업무 처리를 지원하고 창의적인 이미지 생성 및 고해상도 전환 기능을 제공합니다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"삼성 가우스에 대해 설명해주세요\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b9b0df",
   "metadata": {},
   "source": [
    "> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 12)\n",
    "\n",
    "- LangSmith: https://smith.langchain.com/public/2b2913c9-6b9c-4a19-bb16-dc2256e2fdbf/r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05d3d293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027년 AI 소프트웨어 매출은 2,510 억 달러로 예상되며, AI 애플리케이션은 2027년까지 21.1%의 연평균 성장률을 기록할 것으로 전망됩니다. AI 소프트웨어 시장은 AI 플랫폼, AI 애플리케이션, AI 시스템 인프라 소프트웨어를 포괄하고 있습니다.생성 AI 플랫폼과 애플리케이션은 2027년까지 283억 달러의 매출을 창출할 것으로 예상됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"미래의 AI 소프트웨어 매출 전망은 어떻게 되나요?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b388974",
   "metadata": {},
   "source": [
    "> 문서: data/SPRI_AI_Brief_2023년12월호\\_F.pdf (페이지 14)\n",
    "\n",
    "- LangSmith: https://smith.langchain.com/public/17ef6df2-b012-4f8e-b0a8-62894d82c097/r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "265b7d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유튜브는 2024년부터 AI 생성 콘텐츠에 AI 라벨 표시를 의무화했으며, 이를 준수하지 않는 콘텐츠는 삭제되고 크리에이터에 대한 수익 배분도 중단될 수 있습니다. 또한, AI 생성 콘텐츠가 신원 파악이 가능한 개인을 모방한 경우 개인정보 침해 신고 절차에 따라 콘텐츠 삭제 요청도 받을 계획입니다. 유튜브는 AI를 이용한 콘텐츠의 변경이나 합성 여부를 시청자에게 전달하기 위해 라벨을 표시하는 방식을 도입할 예정입니다.\n"
     ]
    }
   ],
   "source": [
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"YouTube 가 2024년에 의무화 한 것은 무엇인가요?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-MAN8Kwi6-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
