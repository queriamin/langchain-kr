{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://dl.acm.org/doi/10.1145/3173574.3174223\n",
      "문서의 수: 1\n",
      "============================================================\n",
      "[HUMAN]\n",
      "이 논문에서 만든 프로토타입은?\n",
      "\n",
      "[AI]\n",
      "AI avatars.\n"
     ]
    }
   ],
   "source": [
    "# 단계 1: 문서 로드(Load Documents)\n",
    "# 뉴스기사 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "url = \"https://dl.acm.org/doi/10.1145/3173574.3174223\"\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url,),\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(\"div\")\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩 & 벡터스토어 생성(Create Vectorstore)\n",
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# 단계 4: 검색(Search)\n",
    "# 뉴스에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 단계 5: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 단계 6: 언어모델 생성(Create LLM)\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 단계 7: 체인 생성(Create Chain)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"이 논문에서 만든 프로토타입은?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"URL: {url}\")\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "print(\"===\" * 20)\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://dl.acm.org/doi/10.1145/3173574.3174223'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdvanced Search\\n\\nBrowse\\n\\nAbout\\n\\n\\n\\n\\n\\n                Sign in\\n            \\n\\n\\n\\n                        Register\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\nAdvanced SearchJournalsMagazinesProceedingsBooksSIGsConferencesPeopleMore\\nSearch ACM Digital LibrarySearchSearch\\nAdvanced Search\\n\\n\\n\\n10.1145/3173574.3174223acmconferencesArticle/Chapter ViewAbstractPublication PageschiConference Proceedingsconference-collectionschiConferenceProceedingsUpcoming EventsAuthorsAffiliationsAward WinnersMore\\n\\n\\n\\nHomeConferencesCHIProceedingsCHI \\'18I Lead, You Help but Only with Enough Details: Understanding User Experience of Co-Creation with Artificial Intelligence\\n\\n\\n\\n\\n\\n\\nExport CitationsSelect Citation formatBibTeXEndNoteACM RefPlease download or close your previous search result export first before starting a new bulk export.Preview is not available.By clicking download,a status dialog will open to start the export process. The process may takea few minutes but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.Download citationCopy citation\\nresearch-article\\nShare on\\nI Lead, You Help but Only with Enough Details: Understanding User Experience of Co-Creation with Artificial IntelligenceAuthors: Changhoon Oh, Jungwoo Song, Jinhan Choi, Seonghyeon Kim, Sungwoo Lee, Bongwon SuhAuthors Info & ClaimsCHI \\'18: Proceedings of the 2018 CHI Conference on Human Factors in Computing SystemsPaper No.: 649, Pages 1 - 13https://doi.org/10.1145/3173574.3174223Published: 21 April 2018 Publication History\\n171citation4,991DownloadsMetricsTotal Citations171Total Downloads4,991Last 12 Months880Last 6 weeks107\\n\\nGet Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account\\nGet AccessContentsCHI \\'18: Proceedings of the 2018 CHI Conference on Human Factors in Computing SystemsI Lead, You Help but Only with Enough Details: Understanding User Experience of Co-Creation with Artificial IntelligencePages 1 - 13PREVIOUS CHAPTEREffects of Viewing Multiple Viewpoint Videos on Metacognition of Collaborative ExperiencesPreviousNEXT CHAPTERSupporting Collaborative Health Tracking in the HospitalNextAbstractSupplementary MaterialReferences\\n\\n\\n\\nInformation & ContributorsBibliometrics & CitationsGet AccessReferencesFiguresTablesMediaShareAbstractRecent advances in artificial intelligence (AI) have increased the opportunities for users to interact with the technology. Now, users can even collaborate with AI in creative activities such as art. To understand the user experience in this new user--AI collaboration, we designed a prototype, DuetDraw, an AI interface that allows users and the AI agent to draw pictures collaboratively. We conducted a user study employing both quantitative and qualitative methods. Thirty participants performed a series of drawing tasks with the think-aloud method, followed by post-hoc surveys and interviews. Our findings are as follows: (1) Users were significantly more content with DuetDraw when the tool gave detailed instructions. (2) While users always wanted to lead the task, they also wanted the AI to explain its intentions but only when the users wanted it to do so. (3) Although users rated the AI relatively low in predictability, controllability, and comprehensibility, they enjoyed their interactions with it during the task. Based on these findings, we discuss implications for user interfaces where users can collaborate with AI in creative works.Supplementary Materialsuppl.mov (pn4885-file5.mp4)Supplemental videoDownload8.36 MBReferences[1]William Albert and Thomas Tullis. 2013. Measuring the user experience: collecting, analyzing, and presenting usability metrics. Newnes.Digital LibraryGoogle Scholar[2]Nick Babich. 2016. 5 Essential UX Rules for Dialog Design. (2016). Retrieved September 18, 2017 from http: //babich.biz/5-essential-ux-rules-for-dialog-design/.Google Scholar[3]Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, and others. 2016. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316 (2016).Google Scholar[4]Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. 2015. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349 (2015).Google Scholar[5]Alex J Champandard. 2016. Semantic style transfer and turning two-bit doodles into fine artworks. arXiv preprint arXiv:1603.01768 (2016).Google Scholar[6]Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. 2015. Deepdriving: Learning affordance for direct perception in autonomous driving. In Proceedings of the IEEE International Conference on Computer Vision. 2722--2730.Digital LibraryGoogle Scholar[7]Keunwoo Choi, George Fazekas, and Mark Sandler. 2016. Text-based LSTM networks for automatic music composition. arXiv preprint arXiv:1604.05358 (2016).Google Scholar[8]John Collier. 1957. Photography in anthropology: a report on two experiments. American anthropologist 59, 5 (1957), 843--859.Google Scholar[9]Nicholas Davis, Chih-Pin Hsiao, Kunwar Yashraj Singh, and Brian Magerko. 2016a. Co-creative drawing agent with object recognition. In Twelfth Artificial Intelligence and Interactive Digital Entertainment Conference.Digital LibraryGoogle Scholar[10]Nicholas Davis, Chih-PIn Hsiao, Kunwar Yashraj Singh, Lisa Li, and Brian Magerko. 2016b. Empirically studying participatory sense-making in abstract drawing with a co-creative cognitive agent. In Proceedings of the 21st International Conference on Intelligent User Interfaces. ACM, 196--207.Digital LibraryGoogle Scholar[11]Umer Farooq, Jonathan Grudin, Ben Shneiderman, Pattie Maes, and Xiangshi Ren. 2017. Human Computer Integration versus Powerful Tools. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems. ACM, 1277--1282.Digital LibraryGoogle Scholar[12]Leon A Gatys, Alexander S Ecker, and Matthias Bethge. 2015. A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576 (2015).Google Scholar[13]Leon A Gatys, Alexander S Ecker, Matthias Bethge, Aaron Hertzmann, and Eli Shechtman. 2016. Controlling perceptual factors in neural style transfer. arXiv preprint arXiv:1611.07865 (2016).Google Scholar[14]Barney Glaser. 2017. Discovery of grounded theory: Strategies for qualitative research. Routledge.Google Scholar[15]Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 (2013).Google Scholar[16]Hayit Greenspan, Bram van Ginneken, and Ronald M Summers. 2016. Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique. IEEE Transactions on Medical Imaging 35, 5 (2016), 1153--1159.CrossrefGoogle Scholar[17]David Ha and Douglas Eck. 2017. A Neural Representation of Sketch Drawings. arXiv preprint arXiv:1704.03477 (2017).Google Scholar[18]Melanie Hartmann. 2009. Challenges in Developing User-Adaptive Intelligent User Interfaces. In LWA. Citeseer, ABIS--6.Google Scholar[19]Rex Hartson and Pardha S Pyla. 2012. The UX Book: Process and guidelines for ensuring a quality user experience. Elsevier.Digital LibraryGoogle Scholar[20]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision. 1026--1034.Digital LibraryGoogle Scholar[21]Eric Horvitz. 1999. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems. ACM, 159--166.Digital LibraryGoogle Scholar[22]Allen Huang and Raymond Wu. 2016. Deep learning for music. arXiv preprint arXiv:1606.04930 (2016).Google Scholar[23]Anthony David Jameson. 2009. Understanding and dealing with usability side effects of intelligent processing. AI Magazine 30, 4 (2009), 23.Digital LibraryGoogle Scholar[24]Jonas Jongejan, Henry Rowley, Takashi Kawashima, Jongmin Kim, and Nick Fox-Gieg. 2017. Quick, Draw! (2017). Retrieved September 18, 2017 from https://quickdraw.withgoogle.com.Google Scholar[25]Jerry Kaplan. 2016. Artificial intelligence: think again. Commun. ACM 60, 1 (2016), 36--38.Digital LibraryGoogle Scholar[26]Aniket Kittur, Jeffrey V Nickerson, Michael Bernstein, Elizabeth Gerber, Aaron Shaw, John Zimmerman, Matt Lease, and John Horton. 2013. The future of crowd work. In Proceedings of the 2013 conference on Computer supported cooperative work. ACM, 1301--1318.Digital LibraryGoogle Scholar[27]Will Knight. 2016. The Dark Secret at the Heart of AI. MIT Technology Review. (2016). Retrieved September 18, 2017 from https://www.technologyreview.com/s/ 604087/the-dark-secret-at-the-heart-of-ai/.Google Scholar[28]Bart P Knijnenburg, Martijn C Willemsen, Zeno Gantner, Hakan Soncu, and Chris Newell. 2012. Explaining the user experience of recommender systems. User Modeling and User-Adapted Interaction 22, 4--5 (2012), 441--504.Digital LibraryGoogle Scholar[29]Google Creative Lab. 2017. AutoDraw. (2017). Retrieved September 18, 2017 from https://experiments.withgoogle.com/chrome/autodraw.Google Scholar[30]Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. 2015. Human-level concept learning through probabilistic program induction. Science 350, 6266 (2015), 1332--1338.Google Scholar[31]Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature 521, 7553 (2015), 436--444.Google Scholar[32]Honglak Lee, Peter Pham, Yan Largman, and Andrew Y Ng. 2009. Unsupervised feature learning for audio classification using convolutional deep belief networks. In Advances in neural information processing systems. 1096--1104.Digital LibraryGoogle Scholar[33]Tuck Leong, Steve Howard, and Frank Vetere. 2008. Choice: abidcating or exercising?. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 715--724.Digital LibraryGoogle Scholar[34]Clayton Lewis and John Rieman. 1993. Task-centered user interface design. A Practical Introduction (1993).Google Scholar[35]Arnold M Lund. 2001. Measuring Usability with the USE Questionnaire12. Usability interface 8, 2 (2001), 3--6.Google Scholar[36]Yotam Mann. 2017. AI Duet. (2017). Retrieved September 18, 2017 from https://experiments.withgoogle.com/ai/ai-duet.Google Scholar[37]Lauren McCarthy. 2017. p5.js. (2017). Retrieved September 18, 2017 from https://github.com/processing/p5.js?files=1.Google Scholar[38]Ian Millington and John Funge. 2016. Artificial intelligence for games. CRC Press.Digital LibraryGoogle Scholar[39]Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, and others. 2015. Human-level control through deep reinforcement learning. Nature 518, 7540 (2015), 529--533.Google Scholar[40]Clifford Nass, Jonathan Steuer, Ellen Tauber, and Heidi Reeder. 1993. Anthropomorphism, agency, and ethopoeia: computers as social actors. In INTERACT\\'93 and CHI\\'93 conference companion on Human factors in computing systems. ACM, 111--112.Digital LibraryGoogle Scholar[41]Changhoon Oh, Taeyoung Lee, Yoojung Kim, SoHyun Park, Bongwon Suh, and others. 2017. Us vs. Them: Understanding Artificial Intelligence Technophobia over the Google DeepMind Challenge Match. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 2523--2534.Digital LibraryGoogle Scholar[42]Amanda Purington, Jessie G Taft, Shruti Sannon, Natalya N Bazarova, and Samuel Hardman Taylor. 2017. Alexa is my new BFF: Social Roles, User Satisfaction, and Personification of the Amazon Echo. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems. ACM, 2853--2859.Digital LibraryGoogle Scholar[43]Mengye Ren, Ryan Kiros, and Richard Zemel. 2015. Exploring models and data for image question answering. In Advances in neural information processing systems. 2953--2961.Digital LibraryGoogle Scholar[44]Xiangshi Ren. 2016. Rethinking the Relationship between Humans and Computers. IEEE Computer 49, 8 (2016), 104--108.Digital LibraryGoogle Scholar[45]Yvonne Rogers, Helen Sharp, and Jenny Preece. 2011. Interaction design: beyond human-computer interaction. John Wiley & Sons.Digital LibraryGoogle Scholar[46]Jürgen Schmidhuber. 2015. Deep learning in neural networks: An overview. Neural networks 61 (2015), 85--117.Digital LibraryGoogle Scholar[47]Ben Shneiderman, Gerhard Fischer, Mary Czerwinski, Mitch Resnick, Brad Myers, Linda Candy, Ernest Edmonds, Mike Eisenberg, Elisa Giaccardi, Tom Hewett, and others. 2006. Creativity support tools: Report from a US National Science Foundation sponsored workshop. International Journal of Human-Computer Interaction 20, 2 (2006), 61--77.CrossrefGoogle Scholar[48]Ben Shneiderman and Pattie Maes. 1997. Direct manipulation vs. interface agents. interactions 4, 6 (1997), 42--61.Digital LibraryGoogle Scholar[49]David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, and others. 2016. Mastering the game of Go with deep neural networks and tree search. Nature 529, 7587 (2016), 484--489.Google Scholar[50]Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. 2013. On the importance of initialization and momentum in deep learning. In International conference on machine learning. 1139--1147.Digital LibraryGoogle Scholar[51]Ilya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11). 1017--1024.Digital LibraryGoogle Scholar[52]Luke Swartz. 2003. Why people hate the paperclip: Labels, appearance, behavior, and social responses to user interface agents. Ph.D. Dissertation. Stanford University Palo Alto, CA.Google Scholar[53]Gheorghe Tecuci, Mihai Boicu, and Michael T Cox. 2007. Seven aspects of mixed-initiative reasoning: An introduction to this special issue on mixed-initiative assistants. AI Magazine 28, 2 (2007), 11.Digital LibraryGoogle Scholar[54]Stuart NK Watt. 1997. Artificial societies and psychological agents. In Software Agents and Soft Computing Towards Enhancing Machine Intelligence. Springer, 27--41.Digital LibraryGoogle Scholar[55]Etienne Wenger. 2014. Artificial intelligence and tutoring systems: computational and cognitive approaches to the communication of knowledge. Morgan Kaufmann.Google Scholar[56]Terry Winograd. 2006. Shifting viewpoints: Artificial intelligence and human-computer interaction. Artificial Intelligence 170, 18 (2006), 1256--1258.Digital LibraryGoogle Scholar[57]Optimal Workshop. 2016. Reframer. (2016). Retrieved September 21, 2016 from https://www.optimalworkshop.com/reframer.Google Scholar[58]Anbang Xu, Zhe Liu, Yufan Guo, Vibha Sinha, and Rama Akkiraju. 2017. A New Chatbot for Customer Service on Social Media. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 3506--3510.Digital LibraryGoogle Scholar[59]Yan Xu, Tao Mo, Qiwei Feng, Peilin Zhong, Maode Lai, I Eric, and Chao Chang. 2014. Deep learning of feature representation with multiple instance learning for medical image analysis. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 1626--1630.CrossrefGoogle Scholar[60]Taizan Yonetsuji. 2017. Paint Chaniner. (2017). Retrieved September 18, 2017 from https://github.com/pfnet/PaintsChainer.Google Scholar\\n\\n\\n\\n\\nCited ByView allWang BZhang KChen ZShen LShen XLiu YBian JShen H(2025)An intelligent font generation system based on stroke inference, mitigating production labor and enhancing design experienceExpert Systems with Applications10.1016/j.eswa.2024.125657263(125657)Online publication date: Mar-2025https://doi.org/10.1016/j.eswa.2024.125657Shojaei FShojaei FOsorio Torres JShih P(2024)Insights From Art Therapists on Using AI-Generated Art in Art Therapy: Mixed Methods StudyJMIR Formative Research10.2196/630388(e63038-e63038)Online publication date: 4-Dec-2024https://doi.org/10.2196/63038Myagkova K(2024)DECISION-MAKING IN THE FIELD OF IMPROVING DIGITAL SERVICES BASED ON CUSTOMER EXPERIENCE (UX): ТHE IMPACT OF USING ARTIFICIAL INTELLIGENCE TECHNOLOGYStrategic decisions and risk management10.17747/2618-947X-2024-2-186-19915:2(186-199)Online publication date: 19-Aug-2024https://doi.org/10.17747/2618-947X-2024-2-186-199Show More Cited By\\n\\n\\n\\n\\n\\nIndex Terms\\nI Lead, You Help but Only with Enough Details: Understanding User Experience of Co-Creation with Artificial IntelligenceHuman-centered computing\\n\\n\\n\\n\\n\\n\\n\\nRecommendations\\nFriend, Collaborator, Student, Manager: How Design of an AI-Driven Game Level Editor Affects CreatorsCHI \\'19: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems  \\nMachine learning advances have afforded an increase in algorithms capable of creating art, music, stories, games, and more. However, it is not yet well-understood how machine learning algorithms might best collaborate with people to support creative ...Read More\"Help Me Help the AI\": Understanding How Explainability Can Support Human-AI InteractionCHI \\'23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems    Despite the proliferation of explainable AI (XAI) methods, little is understood about end-users’ explainability needs and behaviors around XAI explanations. To address this gap and contribute to understanding how explainability can support human-AI ...Read MoreHuman-AI Interaction and AI AvatarsHCI International 2023 – Late Breaking Papers  AbstractHuman-Computer Interaction has been evolving rapidly with the advancement of artificial intelligence and metaverse. Human-AI Interaction is a new area in Human-Computer Interaction. In this paper, we look at AI avatars, which are human-like ...Read More\\n\\n\\n\\n\\nComments\\nPlease enable JavaScript to view thecomments powered by Disqus.\\n\\n\\n\\n\\nInformation & ContributorsInformationPublished In\\nCHI \\'18: Proceedings of the 2018 CHI Conference on Human Factors in Computing SystemsApril 20188489  pagesISBN:9781450356206DOI:10.1145/3173574General Chairs: Regan MandrykUniversity of Saskatchewan, Canada,Mark HancockUniversity of Waterloo, Canada,Program Chairs: Mark PerryBrunel University London, UK,Anna CoxUniversity College London, UK\\nCopyright © 2018 ACM.Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from [email\\xa0protected]SponsorsSIGCHI: ACM Special Interest Group on Computer-Human InteractionPublisherAssociation for Computing MachineryNew York, NY, United StatesPublication HistoryPublished: 21 April 2018PermissionsRequest permissions for this article.Request PermissionsCheck for updatesBadgesHonorable MentionAuthor Tagsartificial intelligencehuman computer collaborationhuman-ai interactionQualifiersResearch-articleFunding SourcesInstitute for Information communications Technology PromotioConferenceCHI \\'18Sponsor:SIGCHICHI \\'18: CHI Conference on Human Factors in Computing SystemsApril 21 - 26, 2018Montreal QC, Canada\\nAcceptance RatesCHI \\'18 Paper Acceptance Rate 666 of 2,590 submissions, 26%;          Overall Acceptance Rate 6,199 of 26,314 submissions, 24%More\\n\\nUpcoming Conference\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCHI 2025\\n\\n\\n\\nSponsor:\\n\\nsigchi\\n\\n\\n\\n\\n\\nACM CHI Conference on Human Factors in Computing Systems\\n\\n\\n\\n\\n\\n\\r\\n                                April 26 - May 1, 2025\\r\\n                            \\n\\n\\n\\n\\n\\n\\r\\n                                \\r\\n                                    Yokohama ,\\r\\n                                \\r\\n                                 \\r\\n                                \\r\\n                                    Japan       \\r\\n                                \\r\\n                            \\n\\n\\n\\n\\n\\n\\n\\nContributors\\n\\n\\n\\nOther MetricsView Article MetricsBibliometrics & CitationsBibliometrics\\n\\n                Article Metrics\\n            \\n171Total CitationsView Citations4,991Total DownloadsDownloads (Last 12 months)880Downloads (Last 6 weeks)107Reflects downloads up to 01 Mar 2025\\nOther MetricsView Author MetricsCitations\\nCited ByView allWang BZhang KChen ZShen LShen XLiu YBian JShen H(2025)An intelligent font generation system based on stroke inference, mitigating production labor and enhancing design experienceExpert Systems with Applications10.1016/j.eswa.2024.125657263(125657)Online publication date: Mar-2025https://doi.org/10.1016/j.eswa.2024.125657Shojaei FShojaei FOsorio Torres JShih P(2024)Insights From Art Therapists on Using AI-Generated Art in Art Therapy: Mixed Methods StudyJMIR Formative Research10.2196/630388(e63038-e63038)Online publication date: 4-Dec-2024https://doi.org/10.2196/63038Myagkova K(2024)DECISION-MAKING IN THE FIELD OF IMPROVING DIGITAL SERVICES BASED ON CUSTOMER EXPERIENCE (UX): ТHE IMPACT OF USING ARTIFICIAL INTELLIGENCE TECHNOLOGYStrategic decisions and risk management10.17747/2618-947X-2024-2-186-19915:2(186-199)Online publication date: 19-Aug-2024https://doi.org/10.17747/2618-947X-2024-2-186-199Milasan L(2024)Unveiling the Transformative Potential of AI-Generated Imagery in Enriching Mental Health ResearchQualitative Health Research10.1177/10497323241274767Online publication date: 19-Sep-2024https://doi.org/10.1177/10497323241274767Park HEirich JLuckow ASedlmair M(2024)\"We Are Visual Thinkers, Not Verbal Thinkers!\": A Thematic Analysis of How Professional Designers Use Generative AI Image Generation ToolsProceedings of the 13th Nordic Conference on Human-Computer Interaction10.1145/3679318.3685370(1-14)Online publication date: 13-Oct-2024https://dl.acm.org/doi/10.1145/3679318.3685370Huang HLei HHsu C(2024)Exploring Designer-Generative AI Collaborative Personas: A Case Study on ChatGPTProceedings of the 17th International Symposium on Visual Information Communication and Interaction10.1145/3678698.3687172(1-5)Online publication date: 11-Dec-2024https://dl.acm.org/doi/10.1145/3678698.3687172Wen HLin XLiu RSu C(2024)Enhancing College Students’ AI Literacy through Human-AI Co-Creation: A Quantitative StudyProceedings of the 2024 International Conference on Digital Society and Artificial Intelligence10.1145/3677892.3677913(123-129)Online publication date: 24-May-2024https://dl.acm.org/doi/10.1145/3677892.3677913Ling LChen XWen RLi TLC R(2024)Sketchar: Supporting Character Design and Illustration Prototyping Using Generative AIProceedings of the ACM on Human-Computer Interaction10.1145/36771028:CHI PLAY(1-28)Online publication date: 15-Oct-2024https://dl.acm.org/doi/10.1145/3677102Choi YMoon JKim KHong JKostakos VKay JHoang T(2024)Exploring the Potential of Generative AI in Song-SigningCompanion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing10.1145/3675094.3678378(816-820)Online publication date: 5-Oct-2024https://dl.acm.org/doi/10.1145/3675094.3678378Antony VHuang C(2024)ID.8: Co-Creating Visual Stories with Generative AIACM Transactions on Interactive Intelligent Systems10.1145/367227714:3(1-29)Online publication date: 2-Aug-2024https://dl.acm.org/doi/10.1145/3672277Show More Cited By\\nView Options\\nLogin optionsCheck if you have access through your login credentials or your institution to get full access on this article.Sign inFull AccessGet this Publication\\nView options PDFView or Download as a PDF file.PDF eReaderView online with eReader.eReaderFiguresTablesMediaShareShareShare this Publication linkCopy LinkCopied!Copying failed.Share on social mediaXLinkedInRedditFacebookemailAffiliationsChanghoon OhSeoul National University, Seoul, Rebublic of KoreaView ProfileJungwoo SongSeoul National University, Suwon-si, Gyeonggi-do, Rebublic of KoreaView ProfileJinhan ChoiSeoul National University, Seoul, Rebublic of KoreaView ProfileSeonghyeon KimSeoul National University, Suwon, Gyeonggi-do, Rebublic of KoreaView ProfileSungwoo LeeSeoul National University, Seoul, Rebublic of KoreaView ProfileBongwon SuhSeoul National University, Seoul, Rebublic of KoreaView ProfileDownload PDF\\n\\n\\n\\n\\n\\nView Table of Conten\\n\\n\\n\\nFooter\\n\\n\\n\\n\\n\\n\\n\\n\\nCategories\\n\\nJournals\\nMagazines\\nBooks\\nProceedings\\nSIGs\\nConferences\\nCollections\\nPeople\\n\\n\\n\\n\\nAbout\\n\\nAbout ACM Digital Library\\nACM Digital Library Board\\nSubscription Information\\nAuthor Guidelines\\nUsing ACM Digital Library\\nAll Holdings within the ACM Digital Library\\nACM Computing Classification System\\nAccessibility Statement\\n\\n\\n\\n\\nJoin\\n\\nJoin ACM\\nJoin SIGs\\nSubscribe to Publications\\nInstitutions and Libraries\\n\\n\\n\\n\\nConnect\\n\\nContact us via email\\nACM on Facebook\\nACM DL on X\\nACM on Linkedin\\n\\nSend Feedback\\nSubmit a Bug Report\\n\\n\\n\\n\\n\\n\\n\\n\\nThe ACM Digital Library is published by the Association for Computing Machinery. Copyright © 2025 ACM, Inc.\\n\\nTerms of Usage\\nPrivacy Policy\\nCode of Ethics\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYour Search Results Download Request We are preparing your search results for download ...We will inform you here when the file is ready.Download now!Your Search Results Download RequestYour file of search results citations is now ready.Download now!Your Search Results Download RequestYour search export query has expired. Please try again.\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://dl.acm.org/doi/10.1145/3173574.3174223\n",
      "문서의 수: 1\n",
      "============================================================\n",
      "[HUMAN]\n",
      "이 논문의 저자 이름은??\n",
      "\n",
      "[AI]\n",
      "Changhoon Oh, Jungwoo Song, Jinhan Choi\n"
     ]
    }
   ],
   "source": [
    "# 단계 8: 체인 실행(Run Chain)\n",
    "# 문서에 대한 질의를 입력하고, 답변을 출력합니다.\n",
    "question = \"이 논문의 저자 이름은??\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"URL: {url}\")\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "print(\"===\" * 20)\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUMAN]\n",
      "이 논문의 제목은??\n",
      "\n",
      "[AI]\n",
      "I Lead, You Help but Only with Enough Details: Understanding User Experience of Co-Creation with Artificial Intelligence.\n"
     ]
    }
   ],
   "source": [
    "question = \"이 논문의 제목은??\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUMAN]\n",
      "이 논문을 간단하게 요약해주세요.\n",
      "\n",
      "[AI]\n",
      "이 논문은 인공지능 아바타와 사용자 경험에 대해 다룹니다. (This paper discusses AI avatars and user experience.)\n"
     ]
    }
   ],
   "source": [
    "question = \"이 논문을 간단하게 요약해주세요.\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUMAN]\n",
      "What is the prototype developed in this paper?\n",
      "\n",
      "[AI]\n",
      "The prototype developed in this paper is AI avatars, which are human-like.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the prototype developed in this paper?\" \n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HUMAN]\n",
      "Can you tell me more about DuetDraw?\n",
      "\n",
      "[AI]\n",
      "DuetDraw is an AI interface that allows users and the AI agent to draw pictures collaboratively. A user study found that users were more content with DuetDraw when the tool gave detailed instructions and when the AI explained its intentions only when users wanted it to do so. The study employed both quantitative and qualitative methods to understand the user experience in this new user-AI collaboration.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you tell me more about DuetDraw?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"[HUMAN]\\n{question}\\n\")\n",
    "print(f\"[AI]\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-szXyjQUn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
